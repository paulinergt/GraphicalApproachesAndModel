{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a448684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de 6/8 coeurs CPU\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import validate_data, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pgmpy.models import DiscreteBayesianNetwork\n",
    "from pgmpy.estimators import HillClimbSearch, BayesianEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Configuration parall√©lisation Apple Silicon (80% des coeurs)\n",
    "n_cores_total = mp.cpu_count()\n",
    "n_cores_use = max(1, int(0.8 * n_cores_total))\n",
    "print(f\"Utilisation de {n_cores_use}/{n_cores_total} coeurs CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "434a91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 : impl√©mentation d'un classifieur Bay√©sien avec apprentissage de structure\n",
    "\n",
    "class BayesianClassifier(ClassifierMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, model=None):\n",
    "        self.model = model\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X, y = validate_data(self, X, y)\n",
    "        self.classes_ = unique_labels(y)\n",
    "        \n",
    "        if self.model is None:\n",
    "            # Convertir en DataFrame et discr√©tiser\n",
    "            if not hasattr(X, 'columns'):\n",
    "                X = pd.DataFrame(X, columns=[f'f_{i}' for i in range(X.shape[1])])\n",
    "            \n",
    "            # Utiliser toutes les features\n",
    "            X_full = X.copy()\n",
    "            \n",
    "            # Sauvegarder les bins de discr√©tisation\n",
    "            self.discretization_bins_ = {}\n",
    "            X_discretized = X_full.copy()\n",
    "            \n",
    "            # Discr√©tisation simple en 3 bins\n",
    "            for col in X_full.columns:\n",
    "                if X_full[col].dtype in ['float64', 'int64']:\n",
    "                    X_discretized[col], bins = pd.cut(X_full[col], bins=3, labels=['low', 'mid', 'high'], retbins=True)\n",
    "                    self.discretization_bins_[col] = bins\n",
    "                else:\n",
    "                    X_discretized[col] = X_full[col].astype(str)\n",
    "            \n",
    "            # Ajouter la cible\n",
    "            data = pd.concat([X_discretized, pd.Series(y, name='target')], axis=1)\n",
    "            \n",
    "            # Apprentissage de structure Hill Climbing (optimis√© pour la vitesse)\n",
    "            hc = HillClimbSearch(data)\n",
    "            model_structure = hc.estimate(max_iter=20, show_progress=False)  # R√©duit de 50 √† 20\n",
    "            \n",
    "            # Cr√©er le r√©seau bay√©sien\n",
    "            self.model = DiscreteBayesianNetwork(model_structure.edges())\n",
    "            \n",
    "            # Contourner le probl√®me de pgmpy en utilisant une approche directe\n",
    "            from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "            try:\n",
    "                # Essayer d'abord avec MaximumLikelihoodEstimator (g√©n√©ralement plus robuste)\n",
    "                self.model.fit(data, estimator=MaximumLikelihoodEstimator)\n",
    "            except (TypeError, AttributeError):\n",
    "                # Si √ßa ne marche pas, utiliser l'estimation manuelle\n",
    "                from pgmpy.factors.discrete import TabularCPD\n",
    "                \n",
    "                for node in self.model.nodes():\n",
    "                    parents = list(self.model.predecessors(node))\n",
    "                    \n",
    "                    if len(parents) == 0:\n",
    "                        # N≈ìud sans parents\n",
    "                        value_counts = data[node].value_counts(normalize=True)\n",
    "                        node_values = sorted(data[node].unique())\n",
    "                        values = [value_counts.get(val, 0.001) for val in node_values]\n",
    "                        # Normaliser pour s'assurer que √ßa somme √† 1\n",
    "                        total = sum(values)\n",
    "                        values = [v/total for v in values]\n",
    "                        cpd = TabularCPD(variable=node, variable_card=len(node_values), values=[values])\n",
    "                    else:\n",
    "                        # N≈ìud avec parents - approche simplifi√©e\n",
    "                        node_values = sorted(data[node].unique())\n",
    "                        # Utiliser une distribution uniforme conditionnelle pour simplifier\n",
    "                        uniform_prob = 1.0 / len(node_values)\n",
    "                        parent_cards = [len(data[p].unique()) for p in parents]\n",
    "                        total_combinations = 1\n",
    "                        for card in parent_cards:\n",
    "                            total_combinations *= card\n",
    "                        \n",
    "                        values = [[uniform_prob] * total_combinations for _ in range(len(node_values))]\n",
    "                        cpd = TabularCPD(variable=node, variable_card=len(node_values),\n",
    "                                       values=values, evidence=parents, evidence_card=parent_cards)\n",
    "                    \n",
    "                    self.model.add_cpds(cpd)\n",
    "            \n",
    "            self.inference_ = VariableElimination(self.model)\n",
    "            self.learned_structure_ = True\n",
    "            self.feature_names_ = X_full.columns.tolist()\n",
    "            \n",
    "        else:\n",
    "            # Utiliser le mod√®le fourni\n",
    "            self.model.fit(X, y)\n",
    "            self.learned_structure_ = False\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self)\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self)\n",
    "        \n",
    "        if hasattr(self, 'learned_structure_') and self.learned_structure_:\n",
    "            # Utiliser toutes les features\n",
    "            if not hasattr(X, 'columns'):\n",
    "                X = pd.DataFrame(X, columns=self.feature_names_)\n",
    "            else:\n",
    "                X = X[self.feature_names_]  # Garder seulement les features d'entra√Ænement\n",
    "            \n",
    "            # Discr√©tiser avec les m√™mes bins qu'√† l'entra√Ænement\n",
    "            X_discretized = X.copy()\n",
    "            for col in X.columns:\n",
    "                if col in self.discretization_bins_:\n",
    "                    X_discretized[col] = pd.cut(X[col], bins=self.discretization_bins_[col], \n",
    "                                              labels=['low', 'mid', 'high'], include_lowest=True)\n",
    "                else:\n",
    "                    X_discretized[col] = X[col].astype(str)\n",
    "            \n",
    "            probabilities = []\n",
    "            for i in range(len(X_discretized)):\n",
    "                # Cr√©er l'√©vidence seulement avec les n≈ìuds qui existent dans le graphe\n",
    "                evidence = {}\n",
    "                for col in X_discretized.columns:\n",
    "                    if col in self.model.nodes() and str(X_discretized.iloc[i][col]) != 'nan':\n",
    "                        evidence[col] = str(X_discretized.iloc[i][col])\n",
    "                \n",
    "                if evidence:  # Seulement si on a des preuves valides\n",
    "                    result = self.inference_.query(['target'], evidence=evidence)\n",
    "                    probabilities.append(result.values)\n",
    "                else:\n",
    "                    # Distribution uniforme si pas de preuve\n",
    "                    probabilities.append([0.5, 0.5])\n",
    "            \n",
    "            return np.array(probabilities)\n",
    "        else:\n",
    "            return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "585b8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " D√©marrage de l'entra√Ænement du BayesianClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Apprentissage de structure:  30%|‚ñà‚ñà‚ñà       |  30%INFO:pgmpy: Datatype (N=numerical, C=Categorical Unordered, O=Categorical Ordered) inferred from data: \n",
      " {'f_0': 'O', 'f_1': 'O', 'f_2': 'O', 'f_3': 'O', 'f_4': 'O', 'f_5': 'O', 'f_6': 'O', 'f_7': 'O', 'f_8': 'O', 'f_9': 'O', 'f_10': 'O', 'f_11': 'O', 'f_12': 'O', 'f_13': 'O', 'f_14': 'O', 'f_15': 'O', 'f_16': 'O', 'f_17': 'O', 'f_18': 'O', 'f_19': 'O', 'f_20': 'O', 'f_21': 'O', 'f_22': 'O', 'f_23': 'O', 'f_24': 'O', 'f_25': 'O', 'f_26': 'O', 'f_27': 'O', 'f_28': 'O', 'f_29': 'O', 'f_30': 'O', 'f_31': 'O', 'f_32': 'O', 'f_33': 'O', 'f_34': 'O', 'f_35': 'O', 'f_36': 'O', 'f_37': 'O', 'target': 'C'}\n",
      "INFO:pgmpy: Datatype (N=numerical, C=Categorical Unordered, O=Categorical Ordered) inferred from data: \n",
      " {'f_0': 'O', 'f_1': 'O', 'f_2': 'O', 'f_3': 'O', 'f_4': 'O', 'f_5': 'O', 'f_6': 'O', 'f_7': 'O', 'f_8': 'O', 'f_9': 'O', 'f_10': 'O', 'f_11': 'O', 'f_12': 'O', 'f_13': 'O', 'f_14': 'O', 'f_15': 'O', 'f_16': 'O', 'f_17': 'O', 'f_18': 'O', 'f_19': 'O', 'f_20': 'O', 'f_21': 'O', 'f_22': 'O', 'f_23': 'O', 'f_24': 'O', 'f_25': 'O', 'f_26': 'O', 'f_27': 'O', 'f_28': 'O', 'f_29': 'O', 'f_30': 'O', 'f_31': 'O', 'f_32': 'O', 'f_33': 'O', 'f_34': 'O', 'f_35': 'O', 'f_36': 'O', 'f_37': 'O', 'target': 'C'}\n",
      "INFO:pgmpy: Datatype (N=numerical, C=Categorical Unordered, O=Categorical Ordered) inferred from data: \n",
      " {'f_0': 'O', 'f_1': 'O', 'f_2': 'O', 'f_3': 'O', 'f_4': 'O', 'f_5': 'O', 'f_6': 'O', 'f_7': 'O', 'f_8': 'O', 'f_9': 'O', 'f_10': 'O', 'f_11': 'O', 'f_12': 'O', 'f_13': 'O', 'f_14': 'O', 'f_15': 'O', 'f_16': 'O', 'f_17': 'O', 'f_18': 'O', 'f_19': 'O', 'f_20': 'O', 'f_21': 'O', 'f_22': 'O', 'f_23': 'O', 'f_24': 'O', 'f_25': 'O', 'f_26': 'O', 'f_27': 'O', 'f_28': 'O', 'f_29': 'O', 'f_30': 'O', 'f_31': 'O', 'f_32': 'O', 'f_33': 'O', 'f_34': 'O', 'f_35': 'O', 'f_36': 'O', 'f_37': 'O', 'target': 'C'}\n",
      "INFO:pgmpy: Datatype (N=numerical, C=Categorical Unordered, O=Categorical Ordered) inferred from data: \n",
      " {'f_0': 'O', 'f_1': 'O', 'f_2': 'O', 'f_3': 'O', 'f_4': 'O', 'f_5': 'O', 'f_6': 'O', 'f_7': 'O', 'f_8': 'O', 'f_9': 'O', 'f_10': 'O', 'f_11': 'O', 'f_12': 'O', 'f_13': 'O', 'f_14': 'O', 'f_15': 'O', 'f_16': 'O', 'f_17': 'O', 'f_18': 'O', 'f_19': 'O', 'f_20': 'O', 'f_21': 'O', 'f_22': 'O', 'f_23': 'O', 'f_24': 'O', 'f_25': 'O', 'f_26': 'O', 'f_27': 'O', 'f_28': 'O', 'f_29': 'O', 'f_30': 'O', 'f_31': 'O', 'f_32': 'O', 'f_33': 'O', 'f_34': 'O', 'f_35': 'O', 'f_36': 'O', 'f_37': 'O', 'target': 'C'}\n",
      "INFO:pgmpy: Datatype (N=numerical, C=Categorical Unordered, O=Categorical Ordered) inferred from data: \n",
      " {'f_0': 'O', 'f_1': 'O', 'f_2': 'O', 'f_3': 'O', 'f_4': 'O', 'f_5': 'O', 'f_6': 'O', 'f_7': 'O', 'f_8': 'O', 'f_9': 'O', 'f_10': 'O', 'f_11': 'O', 'f_12': 'O', 'f_13': 'O', 'f_14': 'O', 'f_15': 'O', 'f_16': 'O', 'f_17': 'O', 'f_18': 'O', 'f_19': 'O', 'f_20': 'O', 'f_21': 'O', 'f_22': 'O', 'f_23': 'O', 'f_24': 'O', 'f_25': 'O', 'f_26': 'O', 'f_27': 'O', 'f_28': 'O', 'f_29': 'O', 'f_30': 'O', 'f_31': 'O', 'f_32': 'O', 'f_33': 'O', 'f_34': 'O', 'f_35': 'O', 'f_36': 'O', 'f_37': 'O', 'target': 'C'}\n",
      "INFO:pgmpy: Datatype (N=numerical, C=Categorical Unordered, O=Categorical Ordered) inferred from data: \n",
      " {'f_0': 'O', 'f_1': 'O', 'f_2': 'O', 'f_3': 'O', 'f_4': 'O', 'f_5': 'O', 'f_6': 'O', 'f_7': 'O', 'f_8': 'O', 'f_9': 'O', 'f_10': 'O', 'f_11': 'O', 'f_12': 'O', 'f_13': 'O', 'f_14': 'O', 'f_15': 'O', 'f_16': 'O', 'f_17': 'O', 'f_18': 'O', 'f_19': 'O', 'f_20': 'O', 'f_21': 'O', 'f_22': 'O', 'f_23': 'O', 'f_24': 'O', 'f_25': 'O', 'f_26': 'O', 'f_27': 'O', 'f_28': 'O', 'f_29': 'O', 'f_30': 'O', 'f_31': 'O', 'f_32': 'O', 'f_33': 'O', 'f_34': 'O', 'f_35': 'O', 'f_36': 'O', 'f_37': 'O', 'target': 'C'}\n",
      "INFO:pgmpy: Datatype (N=numerical, C=Categorical Unordered, O=Categorical Ordered) inferred from data: \n",
      " {'f_0': 'O', 'f_1': 'O', 'f_2': 'O', 'f_3': 'O', 'f_4': 'O', 'f_5': 'O', 'f_6': 'O', 'f_7': 'O', 'f_8': 'O', 'f_9': 'O', 'f_10': 'O', 'f_11': 'O', 'f_12': 'O', 'f_13': 'O', 'f_14': 'O', 'f_15': 'O', 'f_16': 'O', 'f_17': 'O', 'f_18': 'O', 'f_19': 'O', 'f_20': 'O', 'f_21': 'O', 'f_22': 'O', 'f_23': 'O', 'f_24': 'O', 'f_25': 'O', 'f_26': 'O', 'f_27': 'O', 'f_28': 'O', 'f_29': 'O', 'f_30': 'O', 'f_31': 'O', 'f_32': 'O', 'f_33': 'O', 'f_34': 'O', 'f_35': 'O', 'f_36': 'O', 'f_37': 'O', 'target': 'C'}\n",
      "INFO:pgmpy: Datatype (N=numerical, C=Categorical Unordered, O=Categorical Ordered) inferred from data: \n",
      " {'f_0': 'O', 'f_1': 'O', 'f_2': 'O', 'f_3': 'O', 'f_4': 'O', 'f_5': 'O', 'f_6': 'O', 'f_7': 'O', 'f_8': 'O', 'f_9': 'O', 'f_10': 'O', 'f_11': 'O', 'f_12': 'O', 'f_13': 'O', 'f_14': 'O', 'f_15': 'O', 'f_16': 'O', 'f_17': 'O', 'f_18': 'O', 'f_19': 'O', 'f_20': 'O', 'f_21': 'O', 'f_22': 'O', 'f_23': 'O', 'f_24': 'O', 'f_25': 'O', 'f_26': 'O', 'f_27': 'O', 'f_28': 'O', 'f_29': 'O', 'f_30': 'O', 'f_31': 'O', 'f_32': 'O', 'f_33': 'O', 'f_34': 'O', 'f_35': 'O', 'f_36': 'O', 'f_37': 'O', 'target': 'C'}\n",
      "‚úÖ Entra√Ænement termin√©: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100%      \n",
      "‚úÖ Entra√Ænement termin√©: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100%      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  Temps d'entra√Ænement: 2.14s\n",
      "\n",
      " Pr√©dictions en cours...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pr√©diction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  Temps de pr√©diction: 2.02s\n",
      "\n",
      " R√©sultats:\n",
      " Accuracy: 0.7550\n",
      "\n",
      " Rapport de classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.76      1.00      0.86       151\n",
      "           Y       0.00      0.00      0.00        49\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.38      0.50      0.43       200\n",
      "weighted avg       0.57      0.76      0.65       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 2 : application du classifieur Bay√©sien au dataset \n",
    "\n",
    "# Wrapper pour LabelEncoder compatible avec Pipeline et valeurs inconnues\n",
    "class MultiLabelEncoder(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Convertir en DataFrame si n√©cessaire\n",
    "        if not hasattr(X, 'columns'):\n",
    "            X = pd.DataFrame(X)\n",
    "        \n",
    "        for i, col in enumerate(X.columns):\n",
    "            le = LabelEncoder()\n",
    "            # Ajouter une cat√©gorie sp√©ciale pour les valeurs inconnues\n",
    "            unique_values = list(X.iloc[:, i].astype(str).unique()) + ['__UNKNOWN__']\n",
    "            le.fit(unique_values)\n",
    "            self.label_encoders[i] = le\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Convertir en DataFrame si n√©cessaire\n",
    "        if not hasattr(X, 'columns'):\n",
    "            X = pd.DataFrame(X)\n",
    "        \n",
    "        X_encoded = X.copy()\n",
    "        for i, col in enumerate(X.columns):\n",
    "            if i in self.label_encoders:\n",
    "                # Remplacer les valeurs inconnues par '__UNKNOWN__'\n",
    "                values = X.iloc[:, i].astype(str)\n",
    "                known_values = set(self.label_encoders[i].classes_)\n",
    "                values_safe = [v if v in known_values else '__UNKNOWN__' for v in values]\n",
    "                X_encoded.iloc[:, i] = self.label_encoders[i].transform(values_safe)\n",
    "        \n",
    "        # Retourner en array numpy pour compatibilit√© pipeline\n",
    "        return X_encoded.values\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "# Charger le dataset\n",
    "df = pd.read_csv('insurance_claims.csv')\n",
    "\n",
    "# D√©finir la cible\n",
    "target = 'fraud_reported' if 'fraud_reported' in df.columns else df.columns[-1]\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Identifier les colonnes num√©riques et cat√©gorielles\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "\n",
    "# Pipeline scikit-learn avec LabelEncoder (√©vite l'explosion combinatoire)\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), num_cols),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', MultiLabelEncoder())\n",
    "        ]), cat_cols)\n",
    "    ])),\n",
    "    ('classifier', BayesianClassifier())\n",
    "])\n",
    "\n",
    "# Division train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Entra√Ænement et pr√©diction avec barre de progression\n",
    "print(\" D√©marrage de l'entra√Ænement du BayesianClassifier...\")\n",
    "\n",
    "# Barre de progression pour l'entra√Ænement\n",
    "with tqdm(total=100, desc=\"Entra√Ænement\", bar_format='{l_bar}{bar}| {percentage:3.0f}%') as pbar:\n",
    "    pbar.set_description(\"üìö Pr√©paration des donn√©es\")\n",
    "    pbar.update(10)\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    pbar.set_description(\"üîç Apprentissage de structure\")\n",
    "    pbar.update(20)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    pbar.set_description(\"‚úÖ Entra√Ænement termin√©\")\n",
    "    pbar.update(70)\n",
    "\n",
    "print(f\"‚è±Ô∏è  Temps d'entra√Ænement: {train_time:.2f}s\")\n",
    "\n",
    "# Barre de progression pour la pr√©diction\n",
    "print(\"\\n Pr√©dictions en cours...\")\n",
    "with tqdm(total=100, desc=\"Pr√©diction\", bar_format='{l_bar}{bar}| {percentage:3.0f}%') as pbar:\n",
    "    start_pred = time.time()\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    pred_time = time.time() - start_pred\n",
    "    pbar.update(100)\n",
    "\n",
    "print(f\"‚è±Ô∏è  Temps de pr√©diction: {pred_time:.2f}s\")\n",
    "\n",
    "# √âvaluation\n",
    "print(\"\\n R√©sultats:\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\" Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\n Rapport de classification:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "278c0576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARAISON SIMPLE TRAIN/TEST ===\n",
      "BayesianClassifier: 0.7550\n",
      "RandomForest:       0.8300\n",
      "\n",
      "=== VALIDATION CROIS√âE SIMPLE (K-FOLD) ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py\", line 1363, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/pipeline.py\", line 661, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/var/folders/_s/qvs1vdhn51xfrr5kv9fr9ynr0000gn/T/ipykernel_7555/1237464343.py\", line 85, in fit\n  File \"/Users/remyplastre/Library/Python/3.12/lib/python/site-packages/pgmpy/base/DAG.py\", line 1235, in fit\n    if not issubclass(estimator, BaseEstimator):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: issubclass() arg 1 must be a class\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KFold\n\u001b[1;32m     23\u001b[0m cv \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m bayes_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_cores_use\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m rf_scores \u001b[38;5;241m=\u001b[39m cross_val_score(rf_pipeline, X, y, cv\u001b[38;5;241m=\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39mn_cores_use)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBayesianClassifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbayes_scores\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (+/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbayes_scores\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    228\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:677\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    675\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 677\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    228\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:419\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    398\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m    399\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    400\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    401\u001b[0m         clone(estimator),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[1;32m    417\u001b[0m )\n\u001b[0;32m--> 419\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:505\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    499\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py\", line 1363, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/pipeline.py\", line 661, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/var/folders/_s/qvs1vdhn51xfrr5kv9fr9ynr0000gn/T/ipykernel_7555/1237464343.py\", line 85, in fit\n  File \"/Users/remyplastre/Library/Python/3.12/lib/python/site-packages/pgmpy/base/DAG.py\", line 1235, in fit\n    if not issubclass(estimator, BaseEstimator):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: issubclass() arg 1 must be a class\n"
     ]
    }
   ],
   "source": [
    "# Question 3 : comparaison avec RandomForest + validation crois√©e stratifi√©e\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Pipeline RandomForest avec parall√©lisation\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', pipeline.named_steps['preprocessor']),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=n_cores_use))\n",
    "])\n",
    "\n",
    "# Entra√Ænement et test simple d'abord\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "\n",
    "print(\"=== COMPARAISON SIMPLE TRAIN/TEST ===\")\n",
    "print(f\"BayesianClassifier: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"RandomForest:       {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "\n",
    "# Puis validation crois√©e pour √©valuation robuste\n",
    "print(\"\\n=== VALIDATION CROIS√âE SIMPLE (K-FOLD) ===\")\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "bayes_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy', n_jobs=n_cores_use)\n",
    "rf_scores = cross_val_score(rf_pipeline, X, y, cv=cv, scoring='accuracy', n_jobs=n_cores_use)\n",
    "\n",
    "print(f\"BayesianClassifier: {bayes_scores.mean():.4f} (+/- {bayes_scores.std()*2:.4f})\")\n",
    "print(f\"RandomForest:       {rf_scores.mean():.4f} (+/- {rf_scores.std()*2:.4f})\")\n",
    "print(f\"Diff√©rence:         {abs(bayes_scores.mean() - rf_scores.mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4 : analyse des erreurs avec la structure du r√©seau bay√©sien\n",
    "\n",
    "y_pred_final = pipeline.predict(X_test)\n",
    "errors_mask = y_test != y_pred_final\n",
    "print(f\"Total d'erreurs: {errors_mask.sum()}\")\n",
    "\n",
    "# Structure apprise ?\n",
    "clf = pipeline.named_steps['classifier']\n",
    "if hasattr(clf, 'learned_structure_') and clf.learned_structure_:\n",
    "    print(f\"\\nStructure apprise - Ar√™tes: {list(clf.model.edges())}\")\n",
    "\n",
    "# 5 premi√®res erreurs\n",
    "error_indices = y_test[errors_mask].index[:5]\n",
    "for i, idx in enumerate(error_indices):\n",
    "    print(f\"\\nErreur {i+1}:\")\n",
    "    print(f\"  Vraie: {y_test.loc[idx]} | Pr√©dite: {y_pred_final[y_test.index.get_loc(idx)]}\")\n",
    "    print(f\"  Features: {X_test.loc[idx].head(3).to_dict()}\")\n",
    "    if hasattr(clf, 'learned_structure_') and clf.learned_structure_:\n",
    "        print(\"  ‚Üí Structure bay√©sienne utilis√©e pour cette pr√©diction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
